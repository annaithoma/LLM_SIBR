{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOO6LWJoWW/RvHzPF3QXBfz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 2. Extracting topics from research articles\n","In this exercise we will tro to extract taxonomic tags from articles to evaluate the topics and methods used in different article clusters. This time, we will use a text generation approach."],"metadata":{"id":"pvZ8ONQz12UB"}},{"cell_type":"markdown","source":["### **Setting up environment**\n","If you have not done this yet during exercise 1:\n","* Make sure you have a hugging Face account (https://huggingface.co/join).\n","* Go to the meta-llama/Llama-3.2-1B-Instruct model page and fill in the 'COMMUNITY LICENSE AGREEMENT' form at the top of the page to get access to the model (this may take a few minutes).\n","\n","Make sure to set your runtime to use a GPU by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `T4 GPU`"],"metadata":{"id":"zvLy147b2E-5"}},{"cell_type":"code","source":["import sys\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","    # Mount google drive to enable access to data files\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Install requisite packages\n","    !pip install transformers accelerate &> /dev/null\n","\n","    # Change working directory\n","    %cd /content/drive/MyDrive/LLM_SIBR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QehY0Gam2C53","executionInfo":{"status":"ok","timestamp":1750453003819,"user_tz":-120,"elapsed":152067,"user":{"displayName":"Anna","userId":"00021066214241597669"}},"outputId":"691130cc-fd57-4490-c5d1-7eb3c9f4d907"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/LLM_SIBR\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","import re\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.cluster.hierarchy import linkage, fcluster\n","from scipy.spatial.distance import squareform\n","from math import acos, pi\n","from collections import Counter"],"metadata":{"id":"d-Mhq4U7fQU5","executionInfo":{"status":"ok","timestamp":1750452838956,"user_tz":-120,"elapsed":52878,"user":{"displayName":"Anna","userId":"00021066214241597669"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Generating taxonomic tags\n","We will first load the data and concatenate the semantic content of titles and abstracts."],"metadata":{"id":"xn-6udIQivIC"}},{"cell_type":"code","source":["# Load data\n","data = pd.read_csv('data_cleaned_filtered_clustered.csv')"],"metadata":{"id":"hNp72WABi1gz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For time reasons, the code next samples 10 articles from each continent and saves their concatenated titles to `sampled_data` (you can also sample more if you have time to spare in the exercise):"],"metadata":{"id":"xI3A0OI7nMbM"}},{"cell_type":"code","metadata":{"id":"8d6609f0"},"source":["# Sample 10 articles from each continent\n","sampled_data = data.groupby('continent').apply(lambda x: x.sample(n=min(len(x), 10), random_state=42)).reset_index(drop=True)\n","\n","# Concatenate title and abstract\n","sampled_data['text'] = sampled_data['Title'] + ' ' + sampled_data['Abstract_cleaned']\n","\n","# Print the first 5 rows of the new DataFrame to verify the formatting\n","display(sampled_data.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code next loads the LLM and its corresponding tokenizer. We will use \"meta-llama/Llama-3.2-1B-Instruct\", a recent model trained which shows impressive performance given its relatively small size. The smaller size has the main advantage that it can be run on the freely available GPUs on Google Colab.\n","\n","The code begins by setting the random seed. This helps ensure the reproducibility of the often stochastic processes involved in training and running LLMs. It next asks you to provide your Hugging Face access token. Please generate a token by clicking '+ Create new token' > 'Read' > 'Create token' on https://huggingface.co/settings/tokens. You will then need to copy-paste the token into 'your_access_token_here' in the code below in order to download the model.** The code then loads the model and tokenizer. The model is loaded onto the GPU via device_map=\"cuda\" and the model is set to use half-precision via torch_dtype=torch.float16 to save memory (RAM). The trust_remote_code=True argument is used to trust the remote code, and attn_implementation='eager' is used for faster inference on the T4 GPUs available on Google Colab.\n","\n","Troubleshooting: If you receive an error about using a GPU when running the code below, you might still have to click `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `T4 GPU`"],"metadata":{"id":"M07Tt-zioucS"}},{"cell_type":"code","source":["torch.random.manual_seed(42) # For reproducibility\n","\n","access_token = 'your_access_token_here'\n","\n","model_ckpt = 'meta-llama/Llama-3.2-1B-Instruct'\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_ckpt,\n","    device_map=\"cuda\", # Use GPU\n","    torch_dtype=torch.float16, # Use half-precision\n","    trust_remote_code=True,\n","    attn_implementation='eager', # For faster inference on T4 GPUs\n","    token=access_token\n",")\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_ckpt,\n","    token=access_token,\n",")"],"metadata":{"id":"dO_778Eoo8IX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code next initializes a `transformers` pipeline for text generation. This is a high-level API that allows for easy text generation using the pre-trained models. We will use this pipeline to characterize the clusters. The pipeline takes two arguments in addition to the task (`\"text-generation\"`):\n","\n","1. `model`: The model to use for text generation.\n","2. `tokenizer`: The tokenizer to use for text generation."],"metadata":{"id":"VvaT9fE9stte"}},{"cell_type":"code","source":["# Initialize pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n",")"],"metadata":{"id":"n2kVFNhnsubW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We next set our text generation hyperparameters in `generation_args`, which is later feed into the text generation `pipe`. We will be asking the model to produce 3-5 main tags characterizing each cluster.\n","Since we only want the model to output a few tags, the code provides a hard constraint on the generation by setting `\"max_new_tokens\": 150` and `\"do_sample\": False`. It also sets `\"temperature\"` and `\"top_p\"` to `None`, since these parameters do not apply when `do_sample=False`. The `pad_token_id` is set to the end-of-sequence token ID, which is recommended when using the Hugging Face pipeline for text generation:"],"metadata":{"id":"lSoWE261s7Fq"}},{"cell_type":"code","source":["# Text generation arguments\n","generation_args = {\n","    \"max_new_tokens\": 150,\n","    \"return_full_text\": False,\n","    \"do_sample\": False,\n","    \"temperature\": None,\n","    \"top_p\": None,\n","    \"pad_token_id\": pipe.tokenizer.eos_token_id\n","}"],"metadata":{"id":"qAD2aqUotO2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code next initializes our model prompts. Conventionally, the system_prompt instructs the model with a general \"vibe\"/persona that it should take on, whilst the user_prompt_template provides the task-specific instructions. However, there are no strict rules for what you should put in the system versus user prompt: what works and doesn't work depends on the specific assistant-oriented fine-tuning regime the model went through, which vary from model to model. Curly brackets {} act as a placeholder in the prompt that can be filled with an article's title and abstract.\n","\n","## TASK 1: Develop and implement an effective system and user prompt.\n","### System prompt\n","* The system prompt is typically rather short. For example, if you describe the task and character of a very helpful research assistant to another person in one or two sentences, this could already be close to a good system prompt.\n","\n","### User prompt\n","* First, you should describe the task as precise as possible (i.e., analyze the provided article and characterize its core subject and methodology; extract 3-5 non-redundant taxonomic tags)\n","* Next, you can provide more rules for the tag generation, for instance, how specific and concise the tags should be, how strict the model should stick to the text provided, whether it should focus more strongly on the topic or methodology, etc.)\n","* Remember to add curly brackets {} where you want to feed in the text from the article\n","* Lastly, instruct the model about your desired output structure which will make it much easier the process the tags afterwards. In some cases it can be helpful to implement a chain-of-thought process. This means that you ask the model to provide a brief reasoning before generating the tags. Then, tell the model what kind of format you would like to receive your tags in. For example:\n","'Exclusively return the list of the 3-5 generated taxonomic tags. Format the output as a JSON object with the key 'tags'. You are not allowed to add any other text.'.\n","* If you don't want to start from scratch, have a look at the\n","\n","To test how different prompts behave, consider using a subset of data to reduce the time, E.g.,\n","\n","```\n","test_data = sampled_data[0:5]\n","```\n","and replace the data in the line\n","\n","```\n","for index, row in sampled_data.iterrows():\n","```\n","\n","\n"],"metadata":{"id":"mDht4yF_tWCx"}},{"cell_type":"code","source":["# Initialise system prompt and user prompt template\n","system_prompt = \"Add your system prompt here.\"\n","user_prompt_template = \"\"\"\n","Add your user prompt here. Read the instructions above for some hints. Or copy paste the prompt from the cheatsheet to edit.\n","\"\"\"\n","\n","\n","\n","# Iterate through the sampled articles from each continent (their titles and abstracts), generate the model's labels, save to cluster_labels\n","cluster_labels = {}\n","for index, row in sampled_data.iterrows():\n","    continent = row['continent']\n","    text = row['text']\n","\n","    # Insert article titles into the prompt_template\n","    user_prompt = user_prompt_template.format(text)\n","\n","    # JSON format the system and user prompt for feeding into the model\n","    message = [\n","        {'role': 'system', 'content': system_prompt},\n","        {'role': 'user', 'content': user_prompt}\n","    ]\n","\n","    # Generate text and access output at index 0 at key 'generated_text'\n","    output = pipe(message, **generation_args)[0]['generated_text']\n","\n","    # Save output as dataframe  and add the index number and continent information\n","    cluster_labels[index] = {'index': index, 'continent': continent, 'output': output}\n","\n","cluster_labels = pd.DataFrame(cluster_labels).T\n","\n","display(cluster_labels.head())"],"metadata":{"id":"kBshxztBtmIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because we limited the number of tokens, we may miss the closing brackets for the json object that the model is procuding when the model produces nonsense (which may happen once in a while)."],"metadata":{"id":"FoEdArBDYXbl"}},{"cell_type":"code","source":["# If the LLM tag generation didn't work out for you for some reason, you can open tags from a file\n","# remove the hashtag and run the code below\n","# cluster_labels =  pd.read_csv('cluster_labels.csv')"],"metadata":{"id":"PxF5tNBMG3pn","executionInfo":{"status":"ok","timestamp":1750453031160,"user_tz":-120,"elapsed":1089,"user":{"displayName":"Anna","userId":"00021066214241597669"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#let's make sure the json output is properly closed\n","cluster_labels['output_clean'] = cluster_labels['output'].apply(\n","    lambda x: x if re.search(r'\"\\s*\\]\\s*\\}$', x.strip()) else x.strip() + '\" ] }'\n",")\n","\n","display(cluster_labels.head())"],"metadata":{"id":"k7d3eV2dWK6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try to clean up the output, you will need to end up with a column that contains the tags as a list\n","cluster_labels['tags'] = cluster_labels['output_clean'].apply(lambda x: json.loads(x)['tags'])\n","\n","# check what it looks like\n","display(cluster_labels.head())\n","\n","# flatten the tag list and calculate counts\n","tag_counts = Counter(tag for tags in cluster_labels['tags'] for tag in tags)\n","tags_unique = list(tag_counts.keys())\n"],"metadata":{"id":"xOmy8RITFxq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering tags\n","Because we may find considerable redundancy and semantic variation among the tags, we will cluster semantically similar tags together. For this purpose, we will first use an embedding model to extract the semantic embeddings of the tags. We will then use hierarchical clustering methods to group similar tags together based on the cosine similarity of the tags and assign the most frequent tag as the group label.\n","\n","We'll start by embedding the tags (for better performance in an environment that is not as RAM constrained, you can also use other models like 'Salesforce/SFR-Embedding-Mistral')."],"metadata":{"id":"B7WsPtu0QAoE"}},{"cell_type":"code","source":["# Load the SentenceTransformer model\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Create instruct-style prompts for embedding\n","prompts = [f\"Instruct: Embed the distinct meaning of this behavioral reinforcement learning concept.\\nQuery: {tag}\"\n","           for tag in tags_unique]\n","\n","# Encode tags using the model\n","tag_emb = model.encode(prompts, convert_to_numpy=True)\n","tag_emb_df = pd.DataFrame(tag_emb, index=tags_unique)\n"],"metadata":{"id":"ldyZYOyeSYsU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will compute the cosine similarity of the embeddings and normalize the range. Finally, we compute a high similarity cutoff at the 80th percentile of these values, adjusted slightly by a tiny epsilon for precision."],"metadata":{"id":"Dfs_29ewSr7W"}},{"cell_type":"code","source":["# Cosine similarity\n","tag_cos = cosine_similarity(tag_emb_df)\n","np.fill_diagonal(tag_cos, 1.0)\n","\n","# Normalize cosine similarity to range [0,1] using acos\n","tag_cos[tag_cos > 1] = 1\n","tag_cos[tag_cos < -1] = -1\n","tag_cos = 1 - np.arccos(tag_cos) / pi\n","\n","# Set a high similarity cutoff\n","eps = 1 / (20 ** 10)\n","triu_vals = tag_cos[np.triu_indices_from(tag_cos, k=1)]\n","cutoff = np.quantile(triu_vals, 0.8) - eps"],"metadata":{"id":"JXCVDqQ8S3Tt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next step, and similar to exercise 1, we will apply hierarchical clustering to identify which tags should be grouped together because the may have a similar meaning."],"metadata":{"id":"BrSHPkSOS_Tl"}},{"cell_type":"code","source":["# Convert similarity to distance\n","distance_matrix = 1 - tag_cos\n","condensed_dist = squareform(distance_matrix, checks=False)\n","linkage_matrix = linkage(condensed_dist, method='complete')\n","\n","# Cut tree at dynamic level\n","num_steps = sum(linkage_matrix[:, 2] < (1 - cutoff))\n","num_clusters = len(tags_unique) - num_steps\n","cluster_labels_array = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n","\n","# Map cluster numbers to tags\n","tag_to_cluster = dict(zip(tags_unique, cluster_labels_array))\n","\n","# Group tags into cliques (clusters)\n","tag_cliques = defaultdict(list)\n","for tag, cluster_id in tag_to_cluster.items():\n","    tag_cliques[cluster_id].append(tag)"],"metadata":{"id":"HamuycDzTPby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What we still need to do is to create a new dictionary, assign the most frequent tag in the group as the group label (which, ultimately, is the tag or keyword that we will use to characterize the article content) and apply it back to the dataframe to find out which of the new tag groups appear in what continent."],"metadata":{"id":"VXIOJNB8TQBA"}},{"cell_type":"code","source":["tag_dict = {}\n","for clique_tags in tag_cliques.values():\n","    counts = [tag_counts[t] for t in clique_tags]\n","    label = clique_tags[np.argmax(counts)]\n","    for t in clique_tags:\n","        tag_dict[t] = label\n","\n","# Clean each tag list using the dictionary\n","cluster_labels['clustered_tags'] = cluster_labels['tags'].apply(\n","    lambda tags: list(dict.fromkeys(tag_dict.get(tag, tag) for tag in tags))\n",")"],"metadata":{"id":"5n8R8pNPP_8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next step, we want to evaluate at the level of continents which keywords appear most frequently."],"metadata":{"id":"D2msZeErcddb"}},{"cell_type":"code","source":["# Explode cleaned tags so each tag is a separate row (linked to continent)\n","exploded = cluster_labels.explode('tags_clean')\n","\n","# Group by continent and tag, then count occurrences\n","freq_per_continent = exploded.groupby(['continent', 'tags_clean']).size().reset_index(name='frequency')\n","\n","# sort by continent and frequency descending\n","freq_per_continent = freq_per_continent.sort_values(['continent', 'frequency'], ascending=[True, False])\n","\n","#filter to only keep the tags with a frequency higher than 1 and remove the tag \"reinforcement learning\" (which is somewhat meaningless in this dataset)\n","freq_per_continent = freq_per_continent[\n","    (freq_per_continent['frequency'] > 1) &\n","    (~freq_per_continent['tags_clean'].str.lower().eq('reinforcement learning'))\n","]\n","\n","#look at the outcome\n","freq_per_continent"],"metadata":{"id":"YHRQGNPucsLd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 2\n","How do the resulting clustered keywords change if you adapt the cutoff criterion to a higher or lower level?\n","This is the line that you will have to modify:\n","\n","```\n","cutoff = np.quantile(triu_vals, 0.8) - eps\n","```\n","\n"],"metadata":{"id":"t9P2DG5IfJ1S"}},{"cell_type":"markdown","source":["## Task 3\n","\n","It would be cool to look at the variation in keywords that your analysis produced. Navigate to this survey and enter your most frequently occurring keyword as the label for each continent: https://forms.gle/wADcXjtvgChuWkTX8\n"],"metadata":{"id":"oQ5bFw6jfcIR"}}]}
