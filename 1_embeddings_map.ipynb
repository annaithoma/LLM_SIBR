{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Generating semantic embeddings\n",
        "\n",
        "\n",
        "**A Note on how to use Notebook Environments**\n",
        "\n",
        "* To run a cell, press shift + enter. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
        "* Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting Restart runtime. This will clear all variables and outputs.\n",
        "* The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the print() function.\n",
        "* Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with #).\n",
        "\n",
        "***Preparation for Exercise 2*** (Hugging Face and Meta Llama License)\n",
        "* Make sure you have a hugging Face account (https://huggingface.co/join).\n",
        "* Go to the meta-llama/Llama-3.2-1B-Instruct model page and fill in the 'COMMUNITY LICENSE AGREEMENT' form at the top of the page to get access to the model (this may take a few minutes). We will not need this model in the first exercise but can save some time for later."
      ],
      "metadata": {
        "id": "Gt1g6p-fLK0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "08ShI1UWMMSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
        "    # Mount google drive to enable access to data files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install requisite packages\n",
        "    !pip install sentence_transformers pacmap &> /dev/null\n",
        "\n",
        "    # Change working directory\n",
        "    %cd /content/drive/MyDrive/LLM_SIBR"
      ],
      "metadata": {
        "id": "h12nM29VP3qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames. `sentence_transformers`: Is a package for extracting embeddings/features from text data using transformer-based models. The other packages are used for computations, clustering, and plotting."
      ],
      "metadata": {
        "id": "f3GB2yd_QklI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pacmap\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "9o6kRZzAP4hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Extraction\n",
        "The code begins by loading the data as `pandas.DataFrame` objects. Because we want to evaluate semantic information from articles' titles and abstracts, we first concatenate them into a single column (`'text'`). Run the code below."
      ],
      "metadata": {
        "id": "KqjM_q0pRHbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data with only the desired columns\n",
        "data = pd.read_csv('data_cleaned_filtered_clustered.csv', usecols=['Title', 'Abstract_cleaned', 'Year'])\n",
        "\n",
        "# Concatenate titles and abstracts\n",
        "data['text'] = data['Title'] + '.\\n\\n' + data['Abstract_cleaned']\n",
        "data"
      ],
      "metadata": {
        "id": "hJSTOKyhRJmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an example of a text\n",
        "print(data['text'][30])"
      ],
      "metadata": {
        "id": "Ztb_9mvQSI9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will extract embeddings from the text data, which are numerical representations of the meaning of text, using the sentence-transformers package. Because there are more than 5k articles in the file, we will sample around 1,000 to speed up the process (typically you would want the embeddings for all articles)."
      ],
      "metadata": {
        "id": "DjCmlghyRsJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset articles based on year\n",
        "data_sample = data[data['Year'] > 2021].reset_index(drop=True)\n",
        "\n",
        "#How many articles do we end up with?\n",
        "print(len(data_sample))"
      ],
      "metadata": {
        "id": "wMzcF_fbxLvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code makes use of the 'all-MiniLM-L6-v2' model, which is a small and efficient embedding model, to extract embeddings from the sentences. The model will encode the sentences into 384-dimensional vector representations. The cell will then print the features as a pandas dataframe for easy viewing.\n"
      ],
      "metadata": {
        "id": "yKtpvRwt9__f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding extraction pipeline\n",
        "model_ckpt = 'all-MiniLM-L6-v2'\n",
        "#model_ckpt = 'all-mpnet-base-v2' #this is a somewhat larger model that may take more time\n",
        "model = SentenceTransformer(model_ckpt)\n",
        "\n",
        "# Extract features\n",
        "embeddings = model.encode(data_sample['text'], show_progress_bar=True)\n",
        "embeddings"
      ],
      "metadata": {
        "id": "n1kx0NwmSdsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the article embeddings (and reduce their dimensionality for clustering downstream), the code initializes a `PaCMAP` object and uses it to project the embeddings into 2 components (because we want to create a 2-dimensional map)."
      ],
      "metadata": {
        "id": "gA25e4VwSwYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PaCMAP dimensionality reduction\n",
        "pacmap_model = pacmap.PaCMAP(n_components=2, n_neighbors=50, MN_ratio=3, FP_ratio=10.0, distance=\"angular\")\n",
        "lyt = pacmap_model.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "h_aHMNA9S3Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "We will next perform hierarchical clustering using Euclidean distance. We first computes pairwise distances with 'pdist', then build a linkage matrix using the complete linkage method (which considers the furthest pairwise distance between clusters). Finally, we form clusters by cutting the hierarchy to produce a specified number of clusters (in 't = ...')."
      ],
      "metadata": {
        "id": "G_9V5ij3bqZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hierarchical clustering\n",
        "distance_matrix = pdist(lyt, metric='euclidean')\n",
        "Z = linkage(distance_matrix, method='complete')\n",
        "clustering = fcluster(Z, t=3, criterion='maxclust')  # t = ... indicates the number of clusters"
      ],
      "metadata": {
        "id": "0lWr8oN2bseK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting\n",
        "\n",
        "These clusters can then be visualized:\n"
      ],
      "metadata": {
        "id": "ATaEe9fQchaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "jitter = np.random.normal(scale=.3, size=lyt.shape)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(lyt[:, 0] + jitter[:, 0], lyt[:, 1] + jitter[:, 1], c=clustering, cmap=\"Set1\", s=20)\n",
        "\n",
        "# Label cluster centers\n",
        "for cluster_id in np.unique(clustering):\n",
        "    idx = np.where(clustering == cluster_id)[0]\n",
        "    cx, cy = lyt[idx, 0].mean(), lyt[idx, 1].mean()\n",
        "    plt.text(cx, cy, str(cluster_id), color='black', fontsize=10, ha='center')\n",
        "\n",
        "plt.title(\"PaCMAP + Hierarchical Clustering on Sentence Embeddings\")\n",
        "plt.axis('off')\n",
        "\n",
        "#to save the plot if needed (remove # from code lines)\n",
        "#generate current timestamp\n",
        "timestamp = datetime.now().strftime(\"%H-%M-%S\")\n",
        "\n",
        "#create dynamic filename including time\n",
        "#feel free to choose a more meaningful name\n",
        "filename = f\"clusters_pacmap_{timestamp}.png\"\n",
        "\n",
        "#save plot\n",
        "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uJa5gZi5cip8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks"
      ],
      "metadata": {
        "id": "sUoweeNFTqBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1**: Try out different numbers of clusters and have a look how the map changes (in the clustering = fcluster...() line you will need to change the t = ... argument). What number do you think works well for the visualization?"
      ],
      "metadata": {
        "id": "LwxHeT-ITTNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2:** Sample about 1,000 articles from the beginning of the database and re-run the analysis. How does the map change?\n",
        "\n",
        "You will have to modify this line:\n",
        "\n",
        "```\n",
        "data_sample = data[data['Year'] > 2021].reset_index(drop=True)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sZq9-cj-Tj1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 3:*** Some article clusters sometimes seem appear more isolated than other. What may be the reason for this?"
      ],
      "metadata": {
        "id": "yLloeVcSoYS1"
      }
    }
  ]
}
